{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Vectorization Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####imports section####\n",
    "from IPython.display import Markdown, display\n",
    "import tqdm as notebook_tqdm\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "from getpass import getpass\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "#Imports for RAG Pipeline and Document Store\n",
    "from haystack import Document\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.rankers.transformers_similarity import TransformersSimilarityRanker\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "#Imports for Chat Approach\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.utils import print_streaming_chunk\n",
    "from haystack import Pipeline\n",
    "import gradio as gr\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of DOC: \n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./downloads/Clinch River Breeder Reactor Project Preliminary Safety Analysis Report, Volume 3. - ML082960494.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "print(\"Size of DOC: \\n\", len(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation Store\n",
    "-> Will store the content and the respective embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7b7868fb8342d490639fc67701f670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m doc_embedder \u001b[38;5;241m=\u001b[39m SentenceTransformersDocumentEmbedder(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m doc_embedder\u001b[38;5;241m.\u001b[39mwarm_up()\n\u001b[0;32m---> 14\u001b[0m docs_with_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mdoc_embedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding size: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(docs_with_embeddings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#Write documents -> Ids, Content and embeddings\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/haystack/lib/python3.11/site-packages/haystack/components/embedders/sentence_transformers_document_embedder.py:178\u001b[0m, in \u001b[0;36mSentenceTransformersDocumentEmbedder.run\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m    173\u001b[0m     text_to_embed \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_separator\u001b[38;5;241m.\u001b[39mjoin(meta_values_to_embed \u001b[38;5;241m+\u001b[39m [doc\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuffix\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    176\u001b[0m     texts_to_embed\u001b[38;5;241m.\u001b[39mappend(text_to_embed)\n\u001b[0;32m--> 178\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts_to_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc, emb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(documents, embeddings):\n\u001b[1;32m    186\u001b[0m     doc\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m emb\n",
      "File \u001b[0;32m~/miniconda3/envs/haystack/lib/python3.11/site-packages/haystack/components/embedders/backends/sentence_transformers_backend.py:57\u001b[0m, in \u001b[0;36m_SentenceTransformersEmbeddingBackend.embed\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: List[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[0;32m---> 57\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/haystack/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:565\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m convert_to_numpy:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(all_embeddings, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 565\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mall_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[1;32m    566\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#size of documents\n",
    "range_pages = range(0,len(pages))\n",
    "\n",
    "#Define a DocStore \n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "#Convert to Haystack Docs\n",
    "docs = [Document(content=pages[i].page_content, meta=pages[i].metadata) for i in range_pages]\n",
    "\n",
    "#Embedd documents\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\", normalize_embeddings=True)\n",
    "doc_embedder.warm_up()\n",
    "\n",
    "docs_with_embeddings = doc_embedder.run(docs)\n",
    "\n",
    "print(\"Embedding size: \", len(docs_with_embeddings['documents'][0].embedding))\n",
    "\n",
    "#Write documents -> Ids, Content and embeddings\n",
    "document_store.write_documents(docs_with_embeddings[\"documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a RAG Pipeline\n",
    "- To define a RAG Pipeline we need:\n",
    "    - A Text Embedder, for embedding user queries, these embeddings will be use to compare to the embedding documents;\n",
    "    - A Retriever, task to to retrieve the documents that match the user query;\n",
    "    - Template Prompt, it provides the context;\n",
    "    - Generator, LLM Model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x0000027BAB001010>\n",
       "ðŸš… Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "#Define pipeline components\n",
    "pipe_comp = [\n",
    "    ('text_embedder', SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\", normalize_embeddings =True)),\n",
    "    ('retriever', InMemoryEmbeddingRetriever(document_store=document_store, scale_score = True)),\n",
    "    # ('ranker', TransformersSimilarityRanker(model='cross-encoder/ms-marco-MiniLM-L-6-v2', top_k=25)),\n",
    "    ('prompt_builder', PromptBuilder(template=template)),\n",
    "    ('llm', OpenAIGenerator(model=\"gpt-3.5-turbo\", generation_kwargs = {'temperature': 0.1}))\n",
    "]\n",
    "\n",
    "#Define pipeline\n",
    "basic_rag_pipeline = Pipeline()\n",
    "\n",
    "#1. Add components\n",
    "for comp in pipe_comp:\n",
    "    basic_rag_pipeline.add_component(*comp)\n",
    "\n",
    "\n",
    "# #2. Connect components\n",
    "# #LOGIC: Output -> Input\n",
    "basic_rag_pipeline.connect('text_embedder.embedding', \"retriever.query_embedding\")\n",
    "basic_rag_pipeline.connect('retriever', 'prompt_builder.documents')\n",
    "basic_rag_pipeline.connect('prompt_builder', 'llm')\n",
    "\n",
    "\n",
    "# #With Ranker\n",
    "# basic_rag_pipeline.connect('text_embedder', \"retriever\")\n",
    "# basic_rag_pipeline.connect('retriever', \"ranker\")\n",
    "# basic_rag_pipeline.connect('ranker', 'prompt_builder')\n",
    "# basic_rag_pipeline.connect('prompt_builder', 'llm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad991045efa414fba157495ee4305f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Based on the information provided, natural gas would be considered a sustainable activity if it meets the following criteria:\n",
       "\n",
       "1. Climate change mitigation: The direct greenhouse gas emissions of the activity are lower than 270 gCO2e/kWh.\n",
       "2. Climate change adaptation: The activity complies with the criteria set out in the relevant appendix.\n",
       "3. Sustainable use and protection of water and marine resources: The activity complies with the criteria set out in the relevant appendix.\n",
       "4. Pollution prevention and control: The activity complies with the criteria set out in the relevant appendix.\n",
       "5. Protection and restoration of biodiversity and ecosystems: The activity complies with the criteria set out in the relevant appendix.\n",
       "\n",
       "Additionally, for natural gas to be considered sustainable, it should not cause significant harm to the environment and should contribute substantially to the transition to a circular economy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the conditions for natural gas be considrered a sustainable activity?\"\n",
    "\n",
    "query = {\n",
    "    'text_embedder': {\n",
    "        'text': question\n",
    "    },\n",
    "\n",
    "    'prompt_builder': {\n",
    "        'question': question\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "response = basic_rag_pipeline.run(query)\n",
    "Markdown(response['llm']['replies'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert HayStack Pipeline into a Tool\n",
    "def rag_pipeline_func(query: str):\n",
    "    result = basic_rag_pipeline.run(\n",
    "        {\n",
    "            'text_embedder': {'text': query},\n",
    "            'prompt_builder': {'question': query}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\"reply\": result['llm'][\"replies\"][0]}\n",
    "\n",
    "\n",
    "#Add to tools\n",
    "tools = [\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'rag_pipeline_func',\n",
    "            'description': 'Get information about the documents provided',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'query': {\n",
    "                        'type': 'string',\n",
    "                        'description': \"The query to use in the search. Infer this from the user's message. It should be a question or a statement\",\n",
    "                    }\n",
    "                },\n",
    "                'required': [\"query\"],\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "messages = [\n",
    "    ChatMessage.from_system(\"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"),\n",
    "    ChatMessage.from_user(\"Can you provide description of how activities are considered sustainable?\")\n",
    "\n",
    "]\n",
    "\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-3.5-turbo\", streaming_callback=print_streaming_chunk, generation_kwargs={'temperature': 0.1})\n",
    "response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Name: rag_pipeline_func\n",
      "Function Arguments: {'query': 'How are activities considered sustainable?'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d03b7c16904f6299beea6644e28f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Response: {'reply': 'Activities are considered sustainable if they comply with specific criteria and guidelines outlined in the provided information. This includes measures such as transitioning to a circular economy, protecting and restoring biodiversity and ecosystems, mitigating climate change, adapting to climate change, and preventing pollution. Sustainable activities also involve actions such as reducing waste generation, promoting the reuse and recycling of materials, managing hazardous waste properly, minimizing the use of pesticides and fertilizers, and implementing environmental management systems. Additionally, sustainable activities contribute to the protection and restoration of habitats, ecosystems, and species, and ensure that they do not have significant negative impacts on the environment.'}\n"
     ]
    }
   ],
   "source": [
    "#Parse function calling information\n",
    "function_call = json.loads(response[\"replies\"][0].content)[0]\n",
    "function_name = function_call[\"function\"][\"name\"]\n",
    "function_args = json.loads(function_call[\"function\"][\"arguments\"])\n",
    "print(\"Function Name:\", function_name)\n",
    "print(\"Function Arguments:\", function_args)\n",
    "# Find the correspoding function and call it with the given arguments\n",
    "available_functions = {\"rag_pipeline_func\": rag_pipeline_func}\n",
    "function_to_call = available_functions[function_name]\n",
    "function_response = function_to_call(**function_args)\n",
    "print(\"Function Response:\", function_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, activities are considered sustainable if they comply with specific criteria and guidelines. This includes measures such as transitioning to a circular economy, protecting and restoring biodiversity and ecosystems, mitigating climate change, adapting to climate change, and preventing pollution. Sustainable activities also involve actions such as reducing waste generation, promoting the reuse and recycling of materials, managing hazardous waste properly, minimizing the use of pesticides and fertilizers, and implementing environmental management systems. Additionally, sustainable activities contribute to the protection and restoration of habitats, ecosystems, and species, and ensure that they do not have significant negative impacts on the environment."
     ]
    }
   ],
   "source": [
    "function_message = ChatMessage.from_function(content=json.dumps(function_response), name=function_name)\n",
    "messages.append(function_message)\n",
    "response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat APP Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatMessage(content='[{\"id\": \"call_1DhJZIlvM0VXdvHQw5ubRU1J\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\":\\\\\"Carbon dioxide equivalent emissions for each energy source\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 23, 'prompt_tokens': 96, 'total_tokens': 119}})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b224f723eb46acb4a040fdff14e09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatMessage(content='[{\"id\": \"call_nTmqadiKUZibvcmIT17gqeAB\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\":\\\\\"Information about coal in the provided document\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 22, 'prompt_tokens': 665, 'total_tokens': 687}})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fb22ceaa1e4bb29055733e745c54f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatMessage(content='[{\"id\": \"call_CrGxaINihZZHzZD9kHYMmvqp\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\":\\\\\"Conditions for nuclear energy to be considered sustainable\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 23, 'prompt_tokens': 739, 'total_tokens': 762}})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f58541fea74ce08e9939692109a979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatMessage(content='[{\"id\": \"call_vNQSPKOHX8EELmiwsdNJtVKZ\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\":\\\\\"Conditions for natural gas to be deemed sustainable\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 23, 'prompt_tokens': 1282, 'total_tokens': 1305}})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7856f8458b34530a612ec6c24b625af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CHAT APP\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-3.5-turbo\", generation_kwargs={'temperature': 0.1})\n",
    "response = None\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"Present as much relevant information as possible, be descriptive.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "def chatbot_with_fc(message, history):\n",
    "    messages.append(ChatMessage.from_user(message))\n",
    "    response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})\n",
    "\n",
    "    while True:\n",
    "        # if OpenAI response is a tool call\n",
    "        if response and response[\"replies\"][0].meta[\"finish_reason\"] == \"tool_calls\":\n",
    "            function_calls = json.loads(response[\"replies\"][0].content)\n",
    "            print(response[\"replies\"][0])\n",
    "            for function_call in function_calls:\n",
    "                ## Parse function calling information\n",
    "                function_name = function_call[\"function\"][\"name\"]\n",
    "                function_args = json.loads(function_call[\"function\"][\"arguments\"])\n",
    "\n",
    "                ## Find the correspoding function and call it with the given arguments\n",
    "                function_to_call = available_functions[function_name]\n",
    "                function_response = function_to_call(**function_args)\n",
    "\n",
    "                ## Append function response to the messages list using `ChatMessage.from_function`\n",
    "                messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n",
    "                response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})\n",
    "\n",
    "        # Regular Conversation\n",
    "        else:\n",
    "            messages.append(response[\"replies\"][0])\n",
    "            break\n",
    "    return response[\"replies\"][0].content\n",
    "\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chatbot_with_fc,\n",
    "    examples=[\n",
    "        \"Tell me which energy sources are present in the document?\",\n",
    "        \"Can you describe what is the purpose of this document?\",\n",
    "        \"What are the carbon dioxide equivalent emissions for each energy source?\",\n",
    "    ],\n",
    "    title=\"Ask me about the taxonomy!\",\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
