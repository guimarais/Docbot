{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Vectorization Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####imports section####\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "from getpass import getpass\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "#Imports for RAG Pipeline and Document Store\n",
    "from haystack import Document\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.rankers.transformers_similarity import TransformersSimilarityRanker\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "#Imports for Chat Approach\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.utils import print_streaming_chunk\n",
    "from haystack import Pipeline\n",
    "import gradio as gr\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path ./files/Insight_Platform_API_Views_2_1.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./files/Insight_Platform_API_Views_2_1.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m pages \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_and_split()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of DOC: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(pages))\n",
      "File \u001b[1;32mc:\\Users\\lrguimarais\\AppData\\Local\\miniconda3\\envs\\llm\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:182\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[1;34m(self, file_path, password, headers, extract_images)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpypdf package not found, please install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install pypdf`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    181\u001b[0m     )\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(password\u001b[38;5;241m=\u001b[39mpassword, extract_images\u001b[38;5;241m=\u001b[39mextract_images)\n",
      "File \u001b[1;32mc:\\Users\\lrguimarais\\AppData\\Local\\miniconda3\\envs\\llm\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:116\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[1;34m(self, file_path, headers)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[1;31mValueError\u001b[0m: File path ./files/Insight_Platform_API_Views_2_1.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"./files/Insi\")\n",
    "pages = loader.load_and_split()\n",
    "print(\"Size of DOC: \\n\", len(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation Store\n",
    "-> Will store the content and the respective embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size:  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of documents\n",
    "range_pages = range(0,len(pages))\n",
    "\n",
    "#Define a DocStore \n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "#Convert to Haystack Docs\n",
    "docs = [Document(content=pages[i].page_content, meta=pages[i].metadata) for i in range_pages]\n",
    "\n",
    "#Embedd documents\n",
    "doc_embedder = SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\", normalize_embeddings=True)\n",
    "doc_embedder.warm_up()\n",
    "\n",
    "docs_with_embeddings = doc_embedder.run(docs)\n",
    "\n",
    "print(\"Embedding size: \", len(docs_with_embeddings['documents'][0].embedding))\n",
    "\n",
    "#Write documents -> Ids, Content and embeddings\n",
    "document_store.write_documents(docs_with_embeddings[\"documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a RAG Pipeline\n",
    "- To define a RAG Pipeline we need:\n",
    "    - A Text Embedder, for embedding user queries, these embeddings will be use to compare to the embedding documents;\n",
    "    - A Retriever, task to to retrieve the documents that match the user query;\n",
    "    - Template Prompt, it provides the context;\n",
    "    - Generator, LLM Model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x000001B4348C9780>\n",
       "🚅 Components\n",
       "  - text_embedder: SentenceTransformersTextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "🛤️ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\"\n",
    "#Define pipeline components\n",
    "pipe_comp = [\n",
    "    ('text_embedder', SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\", normalize_embeddings =True)),\n",
    "    ('retriever', InMemoryEmbeddingRetriever(document_store=document_store, scale_score = True)),\n",
    "    # ('ranker', TransformersSimilarityRanker(model='cross-encoder/ms-marco-MiniLM-L-6-v2', top_k=25)),\n",
    "    ('prompt_builder', PromptBuilder(template=template)),\n",
    "    ('llm', OpenAIGenerator(model=\"gpt-3.5-turbo\", generation_kwargs = {'temperature': 0.1}))\n",
    "]\n",
    "\n",
    "#Define pipeline\n",
    "basic_rag_pipeline = Pipeline()\n",
    "\n",
    "#1. Add components\n",
    "for comp in pipe_comp:\n",
    "    basic_rag_pipeline.add_component(*comp)\n",
    "\n",
    "\n",
    "# #2. Connect components\n",
    "# #LOGIC: Output -> Input\n",
    "basic_rag_pipeline.connect('text_embedder.embedding', \"retriever.query_embedding\")\n",
    "basic_rag_pipeline.connect('retriever', 'prompt_builder.documents')\n",
    "basic_rag_pipeline.connect('prompt_builder', 'llm')\n",
    "\n",
    "\n",
    "# #With Ranker\n",
    "# basic_rag_pipeline.connect('text_embedder', \"retriever\")\n",
    "# basic_rag_pipeline.connect('retriever', \"ranker\")\n",
    "# basic_rag_pipeline.connect('ranker', 'prompt_builder')\n",
    "# basic_rag_pipeline.connect('prompt_builder', 'llm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The tables that have downstream information are:\n",
       "- DOWN_SG_UTIL_FACTS\n",
       "- DOWN_UTIL_FACTS\n",
       "- DOWN_UTIL_DAY_FACTS\n",
       "- CM_SCQ_DOWN_FACTS\n",
       "- CM_SCQ_DOWN_DAY_FACTS"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Which tables have downstream information?\"\n",
    "\n",
    "query = {\n",
    "    'text_embedder': {\n",
    "        'text': question\n",
    "    },\n",
    "\n",
    "    'prompt_builder': {\n",
    "        'question': question\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "response = basic_rag_pipeline.run(query)\n",
    "Markdown(response['llm']['replies'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert HayStack Pipeline into a Tool\n",
    "def rag_pipeline_func(query: str):\n",
    "    result = basic_rag_pipeline.run(\n",
    "        {\n",
    "            'text_embedder': {'text': query},\n",
    "            'prompt_builder': {'question': query}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\"reply\": result['llm'][\"replies\"][0]}\n",
    "\n",
    "\n",
    "#Add to tools\n",
    "tools = [\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'rag_pipeline_func',\n",
    "            'description': 'Get information about the documents provided',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'query': {\n",
    "                        'type': 'string',\n",
    "                        'description': \"The query to use in the search. Infer this from the user's message. It should be a question or a statement\",\n",
    "                    }\n",
    "                },\n",
    "                'required': [\"query\"],\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "messages = [\n",
    "    ChatMessage.from_system(\"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"),\n",
    "    ChatMessage.from_user(\"Can you provide description of the table CM_SCQ_UP_FACTS?\")\n",
    "\n",
    "]\n",
    "\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-3.5-turbo\", streaming_callback=print_streaming_chunk, generation_kwargs={'temperature': 0.1})\n",
    "response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Name: rag_pipeline_func\n",
      "Function Arguments: {'query': 'Provide description of the table CM_SCQ_UP_FACTS'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Response: {'reply': 'The table CM_SCQ_UP_FACTS contains Cable Modem RF metrics aggregated for each SC-QAM upstream channel. It includes information such as the CMTS ID, CMTS MAC Domain description, channel center frequency, start and end timestamps, various error values (CER, CCER, SNR, RX), modulation type, microreflection, partial service status, and composite connectivity values (critical, major, minor, normal, unknown). This table provides insights into the performance and connectivity status of cable modems on SC-QAM upstream channels.'}\n"
     ]
    }
   ],
   "source": [
    "#Parse function calling information\n",
    "function_call = json.loads(response[\"replies\"][0].content)[0]\n",
    "function_name = function_call[\"function\"][\"name\"]\n",
    "function_args = json.loads(function_call[\"function\"][\"arguments\"])\n",
    "print(\"Function Name:\", function_name)\n",
    "print(\"Function Arguments:\", function_args)\n",
    "# Find the correspoding function and call it with the given arguments\n",
    "available_functions = {\"rag_pipeline_func\": rag_pipeline_func}\n",
    "function_to_call = available_functions[function_name]\n",
    "function_response = function_to_call(**function_args)\n",
    "print(\"Function Response:\", function_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table CM_SCQ_UP_FACTS contains Cable Modem RF metrics aggregated for each SC-QAM upstream channel. It includes information such as the CMTS ID, CMTS MAC Domain description, channel center frequency, start and end timestamps, various error values (CER, CCER, SNR, RX), modulation type, microreflection, partial service status, and composite connectivity values (critical, major, minor, normal, unknown). This table provides insights into the performance and connectivity status of cable modems on SC-QAM upstream channels."
     ]
    }
   ],
   "source": [
    "function_message = ChatMessage.from_function(content=json.dumps(function_response), name=function_name)\n",
    "messages.append(function_message)\n",
    "response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat APP Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatMessage(content='[{\"id\": \"call_Iju2GnQLJqBMl5coBN81OJT5\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\":\\\\\"Which tables have upstream information?\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 21, 'prompt_tokens': 103, 'total_tokens': 124}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatMessage(content='[{\"id\": \"call_nvhQpPVFBGsqwY1PvMC9Y7MH\", \"function\": {\"arguments\": \"{\\\\\"query\\\\\":\\\\\"Describe the table CM_SCQ_UP_DAY_FACTS\\\\\"}\", \"name\": \"rag_pipeline_func\"}, \"type\": \"function\"}]', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, meta={'model': 'gpt-3.5-turbo-0125', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {'completion_tokens': 25, 'prompt_tokens': 364, 'total_tokens': 389}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.47it/s]\n"
     ]
    }
   ],
   "source": [
    "#CHAT APP\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-3.5-turbo\", generation_kwargs={'temperature': 0.1})\n",
    "response = None\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "def chatbot_with_fc(message, history):\n",
    "    messages.append(ChatMessage.from_user(message))\n",
    "    response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})\n",
    "\n",
    "    while True:\n",
    "        # if OpenAI response is a tool call\n",
    "        if response and response[\"replies\"][0].meta[\"finish_reason\"] == \"tool_calls\":\n",
    "            function_calls = json.loads(response[\"replies\"][0].content)\n",
    "            print(response[\"replies\"][0])\n",
    "            for function_call in function_calls:\n",
    "                ## Parse function calling information\n",
    "                function_name = function_call[\"function\"][\"name\"]\n",
    "                function_args = json.loads(function_call[\"function\"][\"arguments\"])\n",
    "\n",
    "                ## Find the correspoding function and call it with the given arguments\n",
    "                function_to_call = available_functions[function_name]\n",
    "                function_response = function_to_call(**function_args)\n",
    "\n",
    "                ## Append function response to the messages list using `ChatMessage.from_function`\n",
    "                messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))\n",
    "                response = chat_generator.run(messages=messages, generation_kwargs={\"tools\": tools})\n",
    "\n",
    "        # Regular Conversation\n",
    "        else:\n",
    "            messages.append(response[\"replies\"][0])\n",
    "            break\n",
    "    return response[\"replies\"][0].content\n",
    "\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chatbot_with_fc,\n",
    "    examples=[\n",
    "        \"Tell me which tables have upstream information?\",\n",
    "        \"Can you describe the table CM_SCQ_UP_DAY_FACTS?\",\n",
    "        \"Is this a table with upstream or downstream information?\",\n",
    "    ],\n",
    "    title=\"Ask me about the server nxt!\",\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
